{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Youtube Data Cleaning\n",
    "\n",
    "This file was used to clean the Youtube data obtained from Google Takeout. The cleaned data and results are saved as a csv file to be used in sleep_inference.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the watch history data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPLACE WITH YOUR FILE PATH\n",
    "filename = \"../EZ_data/watch-history-json.json\"\n",
    "\n",
    "with open(filename, 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# List to store extracted data\n",
    "extracted_data = []\n",
    "\n",
    "# Loop through each entry in the JSON data and extract the relevant fields\n",
    "for entry in data:\n",
    "    title = entry.get(\"title\", None)\n",
    "    url = entry.get(\"titleUrl\", None)\n",
    "    time_str = entry.get(\"time\", None)\n",
    "\n",
    "    # Try to parse the date (you can adjust the format if needed)\n",
    "    date_time = None\n",
    "    if time_str:\n",
    "        try:\n",
    "            date_time = datetime.strptime(time_str, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "        except ValueError:\n",
    "            date_time = time_str  # Keep original string if parsing fails\n",
    "\n",
    "    # Only append if there is a valid title (to avoid extra records)\n",
    "    if title:\n",
    "        extracted_data.append({\"title\": title, \"url\": url, \"date_time\": date_time})\n",
    "\n",
    "# Convert the extracted data to a DataFrame\n",
    "df_watch = pd.DataFrame(extracted_data)\n",
    "df_watch.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date_time to datetime\n",
    "df_watch['date_time'] = pd.to_datetime(df_watch['date_time'], errors='coerce', utc=True)\n",
    "\n",
    "# Convert from UTC to US/Eastern timezone\n",
    "df_watch['date_time'] = df_watch['date_time'].dt.tz_convert('US/Eastern')\n",
    "\n",
    "# Extract the date and time separately\n",
    "df_watch['date'] = df_watch['date_time'].dt.date\n",
    "df_watch['time'] = df_watch['date_time'].dt.time\n",
    "df_watch.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract title\n",
    "pattern = r'^(Watched)\\s+(.*)$'\n",
    "\n",
    "df_watch[[\"type\", \"title\"]] = df_watch[\"title\"].str.extract(pattern)\n",
    "df_watch.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add videos per day\n",
    "videos_per_day = df_watch.groupby('date').size().reset_index(name='total_videos_watched')\n",
    "videos_per_day['date'] = pd.to_datetime(videos_per_day['date'], errors='coerce')\n",
    "start_date = \"2025-01-01\"\n",
    "end_date = \"2025-02-17\"\n",
    "start_date = pd.to_datetime(start_date).date()\n",
    "start_date = pd.Timestamp(start_date)\n",
    "end_date = pd.to_datetime(end_date).date()\n",
    "end_date = pd.Timestamp(end_date)\n",
    "videos_per_day = videos_per_day[(videos_per_day[\"date\"] >= start_date) & (videos_per_day[\"date\"] <= end_date)]\n",
    "\n",
    "videos_per_day.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the search history data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPLACE WITH YOUR FILE PATH\n",
    "filename = \"../EZ_data/search-history.json\"\n",
    "\n",
    "with open(filename, 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# List to store extracted data\n",
    "extracted_data = []\n",
    "\n",
    "# Loop through each entry in the JSON data and extract the relevant fields\n",
    "for entry in data:\n",
    "    title = entry.get(\"title\", None)\n",
    "    url = entry.get(\"titleUrl\", None)\n",
    "    time_str = entry.get(\"time\", None)\n",
    "\n",
    "    # Try to parse the date (you can adjust the format if needed)\n",
    "    date_time = None\n",
    "    if time_str:\n",
    "        try:\n",
    "            date_time = datetime.strptime(time_str, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "        except ValueError:\n",
    "            date_time = time_str  # Keep original string if parsing fails\n",
    "\n",
    "    # Only append if there is a valid title (to avoid extra records)\n",
    "    if title:\n",
    "        extracted_data.append({\"title\": title, \"url\": url, \"date_time\": date_time})\n",
    "\n",
    "# Convert the extracted data to a DataFrame\n",
    "df_search = pd.DataFrame(extracted_data)\n",
    "df_search.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the search history and watch history using regex\n",
    "pattern = r'^(Searched|Watched)\\s+(.*)$'\n",
    "\n",
    "df_search[[\"type\", \"title\"]] = df_search[\"title\"].str.extract(pattern)\n",
    "df_search.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date_time to datetime\n",
    "df_search['date_time'] = pd.to_datetime(df_search['date_time'], errors='coerce', utc=True)\n",
    "\n",
    "# Convert from UTC to US/Eastern timezone\n",
    "df_search['date_time'] = df_search['date_time'].dt.tz_convert('US/Eastern')\n",
    "\n",
    "# Extract the date and time separately\n",
    "df_search['date'] = df_search['date_time'].dt.date\n",
    "df_search['time'] = df_search['date_time'].dt.time\n",
    "df_search.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine the search and watch history data exported from Google Takeout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the dataframes\n",
    "df = pd.concat([df_search, df_watch], axis=0)\n",
    "\n",
    "# Remove duplicate records\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Sort by date and time and filter out time range\n",
    "start_date = \"2025-01-01\"\n",
    "end_date = \"2025-02-17\"\n",
    "start_date = pd.to_datetime(start_date).date()\n",
    "end_date = pd.to_datetime(end_date).date()\n",
    "df = df[(df[\"date\"] >= start_date) & (df[\"date\"] <= end_date)]\n",
    "df = df.sort_values(\"date_time\")\n",
    "\n",
    "# Extract the day of the week\n",
    "df['day_of_week'] = df['date_time'].dt.day_name()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create summary for analyzing Youtube summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a threshold for a new session; events more than 30 minutes apart indicate a new session\n",
    "session_gap = timedelta(minutes=30)\n",
    "\n",
    "# Compute the difference in time between consecutive events\n",
    "df['time_diff'] = df['date_time'].diff()\n",
    "\n",
    "# Create a boolean column indicating if an event starts a new session\n",
    "df['new_session'] = (df['time_diff'] > session_gap) | (df['time_diff'].isna())\n",
    "\n",
    "# Create a session ID by cumulatively summing the new_session markers\n",
    "df['session_id'] = df['new_session'].cumsum()\n",
    "\n",
    "# Calculate session start, end, and duration by grouping on session_id\n",
    "session_stats = df.groupby('session_id')['date_time'].agg(['min', 'max'])\n",
    "session_stats['session_duration'] = session_stats['max'] - session_stats['min']\n",
    "session_stats['date'] = session_stats['max'].dt.date\n",
    "\n",
    "session_stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the date from the session's end time (max)\n",
    "session_stats['date'] = session_stats['max'].dt.date\n",
    "\n",
    "# For each date, identify the session with the latest end time ('max')\n",
    "last_sessions = session_stats.loc[session_stats.groupby('date')['max'].idxmax()]\n",
    "\n",
    "# Get the session IDs corresponding to the last session of each day\n",
    "last_session_ids = last_sessions.index\n",
    "\n",
    "# Filter the original DataFrame to include only the records from these last sessions\n",
    "df_last_sessions = df[df['session_id'].isin(last_session_ids)]\n",
    "\n",
    "# Now, for each of these last sessions, pick only the final record (i.e. the one with the max timestamp)\n",
    "last_record_per_session = df_last_sessions.groupby('session_id', as_index=False).last()\n",
    "\n",
    "last_session_df = last_record_per_session[['date_time', 'session_id', 'time_diff', 'date']]\n",
    "last_session_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create summary for overall Youtube activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for getting the first and last visits on Youtube \n",
    "def get_first_and_last_visits(df):\n",
    "    # find first and last visited websites per date\n",
    "    first_visits = df.loc[df.groupby(\"date\")[\"time\"].idxmin()]\n",
    "    last_visits = df.loc[df.groupby(\"date\")[\"time\"].idxmax()]\n",
    "\n",
    "    # select relevant columns\n",
    "    first_visits = first_visits[[\"date\", \"time\", \"url\", \"title\", \"type\"]].rename(columns={\n",
    "        \"time\": \"time_first\",\n",
    "        \"url\": \"url_first\",\n",
    "        \"title\": \"title_first\",\n",
    "        \"type\": \"type_first\"\n",
    "    })\n",
    "    \n",
    "    last_visits = last_visits[[\"date\", \"time\", \"url\", \"title\", \"type\"]].rename(columns={\n",
    "        \"time\": \"time_last\",\n",
    "        \"url\": \"url_last\",\n",
    "        \"title\": \"title_last\",\n",
    "        \"type\": \"type_last\"\n",
    "    })\n",
    "\n",
    "    # merge both DataFrames on visit_date to get one row per date\n",
    "    youtube_summary = pd.merge(first_visits, last_visits, on=\"date\")\n",
    "    return youtube_summary\n",
    "\n",
    "youtube_summary_df = get_first_and_last_visits(df)\n",
    "\n",
    "youtube_summary_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the data for the last session each night\n",
    "youtube_summary_df = pd.merge(youtube_summary_df, last_session_df, on=\"date\", how=\"inner\")\n",
    "youtube_summary_df[\"had_session\"] = np.where(youtube_summary_df[\"session_id\"].isna(), False, True)\n",
    "youtube_summary_df = youtube_summary_df.drop_duplicates(subset=['date'])\n",
    "\n",
    "# Rename column for time_diff\n",
    "youtube_summary_df = youtube_summary_df.rename(columns={\"time_diff\": \"session_duration\"})\n",
    "\n",
    "youtube_summary_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Youtube summary with sleep data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import sleep data\n",
    "\n",
    "# CHANGE THIS TO YOUR FILE PATH\n",
    "sleep_filename = \"../elaine_sleep_data.csv\"\n",
    "\n",
    "sleep_df = pd.read_csv(sleep_filename)\n",
    "sleep_df[\"Date\"] = pd.to_datetime(sleep_df[\"Date\"])\n",
    "sleep_df = sleep_df.rename(columns={\"Date\": \"date\"})\n",
    "sleep_df = sleep_df.drop(columns=[\"Unnamed: 0\"])\n",
    "sleep_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_summary_df[\"date\"] = pd.to_datetime(youtube_summary_df[\"date\"])\n",
    "df = pd.merge(sleep_df, youtube_summary_df, on=\"date\", how=\"right\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add videos per day\n",
    "df = pd.merge(df, videos_per_day,  on=\"date\", how=\"right\")\n",
    "df = df[df['total_videos_watched'] <= 180]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RENAME FILE AS APPROPRIATE\n",
    "df.to_csv(\"../youtube_sleep_EZ.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
